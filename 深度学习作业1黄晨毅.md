# <center> 标题

---

## 相关工作

### Transformer

Transformer由谷歌团队2017年发表的论文[Attention is All You Need](https://arxiv.org/abs/1706.03762)提出，如题目所说，Transformer中最重要的就是Attention机制。
<br>传统的NLP任务普遍采用**RNN结构**的序列模型对输入的词向量提取上下文特征，而到2014年，Bengio团队提出的**Attention机制**迅速被运用在深度学习的各个领域，比如计算机视觉中利用Attention捕捉图像的感受野，NLP的RNN模型中也融合了Attention机制，具体表现在Decoder解码时会根据Encoder词向量分配不同的Attention权重。
<br>Transformer抛弃了传统的CNN或RNN结构，整个模型仅由Attention机制以及FeedForward块组成。一个基于Transformer的网络可通过堆叠Transformer块训练，原文堆叠了6个Encoder以及6个Decoder训练，而2018年基于Transformer架构的BERT模型在NLP的11个任务中均取得SOTA，由此可见Transformer的威力。
<br>Transformer中提出了新的Attention机制：**Multihead Self Attention**，SelfAttention即对向量内部做自注意力，利用点积键值对机制计算注意力权重并加权求和，为了防止过拟合并加速收敛，Transformer还对Attention采用多头机制，即每个头分别计算注意力，最后由拼接或相加或线性映射融合多头信息。作者采用Attention代替RNN的原因是RNN只能从左向右或从右向左顺序计算，t时刻的输出依赖于t-1时刻的输出，限制了模型的并行计算能力，同时该特性使模型会遗忘一些长期的依赖信息。

### VGGNet

[VGGNet]VGG网络起源于Simonyan 和Zisserman的文章Very Deep Convolutional Networks for Large Scale Image Recognition，其中VGG是两位作者所在的牛津大学视觉几何组（Visual Geometry Group）的缩写


#### 深度神经网络的退化现象

从经验来看，随着网络层数加深，网络可以提取一些更复杂、更底层的特征，理论上可以取得更好的效果以及更强的泛化能力。12年AlexNet在ILSVRC挑战赛中取得了冠军，且随着VGGNet、Inception v1、v2、v3的提出更进一步证明了这个理论，但实验发现，随着网络层数进一步加深，模型的准确率达到了饱和甚至出现大幅度的下降，这被ResNet团队称为退化现象。根据他们的解释，退化现象的原因是深度神经网络由于引入了过多的非线性变换和激活函数，数据被映射到了过于离散的空间，此时难以让数据回到原点(恒等变换)，因此ResNet团队在深度神经网络中引入shotcut连接增加线性变换，以增加网络的恒等变换能力。

---

## 本文工作

### VGGnet

#### Data Preprocess

输入数据分为train以及val，分别包含带标签的3000条训练数据以及1084条无标签的测试数据。将train按指定比例(默认0.8:0.2)切分为训练集以及验证集。
Resize：
为了统一输入，将图片压缩为256 * 256的三维矩阵，再裁剪其中心的 224 * 224 作为输入数据。
Transforms：
对输入数据进行随机(50%概率)的左右翻转，以及随机角度的旋转(-120°~ 120°)。

#### PatchEmbedding

PatchEmbedding的作用是将图像问题转化为Transformer适用的序列问题。


---

## 实验与分析

### 数据集

我的模型在Bitmoji人脸性别判断数据集上实验，该数据集存在4083条人脸数据，其中3000条标注了性别作为训练集，1083条作为测试集，我将训练集中的20%共600条数据划分为验证集用于每个epoch后验证模型效果。

### 实验设置

我使用VGG16架构,详细参数见下表:

|参数名|值|
|---|---|
|batch_size|128|
|shuffle|True|
|epoch|50|
|learning|0.001|


实验使用Adam作为optimizer并采用学习率余弦下降退火变化,Cross Entropy Loss为损失函数。

### Baseline

本文采用普通的VGG16作为基准模型对比效果。

### Results

实验以Precision、Recall、F1作为指标，在训练50epoch后取验证集F1值最高的模型，其各指标值如下：

|Model|P|R|F1|val_loss|
|---|---|---|---|---|
|VGG|0.4573|**0.4169**|**0.3172**|0.6919|


从实验结果来看，VGG网络对于训练性别分类模型效果不是很好。
